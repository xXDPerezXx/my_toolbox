import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

# # if you want to suppress the warnings
# import warnings
# from scipy.stats import UserWarning

# # Tell Python to ignore this specific warning
# warnings.filterwarnings("ignore", category=UserWarning, message="p-value floored")

# # Now, when you run your scenario, the warning won't be displayed
# run_scenario("SCENARIO 2: COMPARING TWO DIFFERENT DISTRIBUTIONS", data3, data4)

# # It's good practice to reset warnings afterward if you need them elsewhere
# warnings.resetwarnings()

def compare_distributions(
    series1: pd.Series,
    series2: pd.Series,
    bins: int = 30,
    show_kde: bool = True,
    alpha: float = 0.05
):
    """
    Performs a comprehensive comparison of two numeric pandas Series.

    This function provides three key outputs:
    1. A DataFrame with detailed descriptive statistics for each series.
    2. A histogram plot visually comparing the two distributions.
    3. A DataFrame with results from several statistical tests to determine
       if the distributions are significantly different.

    Args:
        series1 (pd.Series): The first numeric series to compare.
        series2 (pd.Series): The second numeric series to compare.
        bins (int, optional): The number of bins to use for the histogram.
                              Defaults to 30.
        show_kde (bool, optional): If True, overlays a Kernel Density Estimate
                                   on the histogram. Defaults to True.
        alpha (float, optional): The significance level for the statistical
                                 tests to determine if the distributions
                                 are different. Defaults to 0.05.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two pandas DataFrames:
        - The first DataFrame contains descriptive statistics.
        - The second DataFrame contains the results of the comparison tests.
        Returns a tuple of (None, error_df) if data validation fails.
    """
    # --- 1. Internal Helper Function for Validation ---
    def _validate_and_convert_series(series: pd.Series) -> pd.Series | None:
        """Validates if a series is numeric or can be converted to numeric."""
        if pd.api.types.is_numeric_dtype(series):
            return series.copy()
        
        converted_series = pd.to_numeric(series, errors='coerce')
        
        if converted_series.isnull().all():
            return None
        return converted_series

    # --- 2. Data Validation and Preparation ---
    s1_validated = _validate_and_convert_series(series1)
    s2_validated = _validate_and_convert_series(series2)

    s1_name = series1.name or "Series 1"
    s2_name = series2.name or "Series 2"

    if s1_validated is None or s2_validated is None:
        failed_series = s1_name if s1_validated is None else s2_name
        error_df = pd.DataFrame({
            "Error": [f"Series '{failed_series}' could not be converted to a numeric type."]
        })
        return None, error_df

    # --- 3. Output 1: Descriptive Statistics DataFrame ---
    stats_dict = {}
    for name, s in [(s1_name, s1_validated), (s2_name, s2_validated)]:
        mode_val = s.mode().iloc[0] if not s.mode().empty else np.nan
        stats_dict[name] = {
            "Non-Null Count": s.count(),
            "Null Count": s.isnull().sum(),
            "Mean": s.mean(),
            "Std Dev": s.std(),
            "Min": s.min(),
            "1% Quantile": s.quantile(0.01),
            "25% Quantile (Q1)": s.quantile(0.25),
            "50% Quantile (Median)": s.quantile(0.50),
            "75% Quantile (Q3)": s.quantile(0.75),
            "99% Quantile": s.quantile(0.99),
            "Max": s.max(),
            "Mode": mode_val
        }
    descriptive_stats_df = pd.DataFrame(stats_dict)

    s1_clean = s1_validated.dropna()
    s2_clean = s2_validated.dropna()
    
    if s1_clean.empty or s2_clean.empty:
        error_df = pd.DataFrame({
            "Error": ["One or both series are empty after removing NaN values. Cannot perform tests."]
        })
        return descriptive_stats_df, error_df

    # --- 4. Output 2: Visualization ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 7))
    sns.histplot(s1_clean, bins=bins, kde=show_kde, color='royalblue', alpha=0.6, label=s1_name)
    sns.histplot(s2_clean, bins=bins, kde=show_kde, color='orange', alpha=0.6, label=s2_name)
    plt.title(f'Distribution Comparison: {s1_name} vs. {s2_name}', fontsize=16)
    plt.xlabel('Value', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.legend()
    plt.show()

    # --- 5. Output 3: Comparison Metrics DataFrame ---
    results_list = []
    
    # Kolmogorov-Smirnov Test
    # **FIXED HERE**
    ks_result = stats.ks_2samp(s1_clean, s2_clean)
    ks_p = ks_result.pvalue
    results_list.append({
        "Metric": "Kolmogorov-Smirnov Test",
        "Result": f"Statistic={ks_result.statistic:.4f}, P-Value={ks_p:.4f}",
        "Interpretation": f"Distributions are likely {'different' if ks_p < alpha else 'similar'} at alpha={alpha}"
    })

    # Mann-Whitney U Test
    # **FIXED HERE**
    mw_result = stats.mannwhitneyu(s1_clean, s2_clean)
    mw_p = mw_result.pvalue
    results_list.append({
        "Metric": "Mann-Whitney U Test",
        "Result": f"Statistic={mw_result.statistic:.4f}, P-Value={mw_p:.4f}",
        "Interpretation": f"Distributions are likely {'different' if mw_p < alpha else 'similar'} at alpha={alpha}"
    })
    
    # Anderson-Darling Test
    ad_stat, ad_crit, ad_sig = stats.anderson_ksamp([s1_clean, s2_clean])
    # The p-value interpretation for AD test is based on comparing the statistic to critical values.
    # We check if the statistic exceeds the critical value at our alpha level (approximated).
    # A common approach is to compare against the 5% critical value.
    try:
        crit_val_5_percent = ad_crit[2]
        interpretation_ad = "different" if ad_stat > crit_val_5_percent else "similar"
        result_str = f"Statistic={ad_stat:.4f} (Crit. Val @5%={crit_val_5_percent:.4f})"
    except IndexError:
        interpretation_ad = "unknown"
        result_str = f"Statistic={ad_stat:.4f} (Could not determine critical value)"

    results_list.append({
        "Metric": "Anderson-Darling Test",
        "Result": result_str,
        "Interpretation": f"Distributions are likely {interpretation_ad} (compared at 5% level)"
    })
    
    # Cramér-von Mises Test
    # **FIXED HERE**
    cvm_result = stats.cramervonmises_2samp(s1_clean, s2_clean)
    cvm_p = cvm_result.pvalue
    results_list.append({
        "Metric": "Cramér-von Mises Test",
        "Result": f"Statistic={cvm_result.statistic:.4f}, P-Value={cvm_p:.4f}",
        "Interpretation": f"Distributions are likely {'different' if cvm_p < alpha else 'similar'} at alpha={alpha}"
    })
    
    # Wasserstein Distance
    w_dist = stats.wasserstein_distance(s1_clean, s2_clean)
    results_list.append({
        "Metric": "Wasserstein Distance",
        "Result": f"Distance={w_dist:.4f}",
        "Interpretation": "The 'cost' to transform one distribution to the other. Smaller is more similar."
    })

    comparison_metrics_df = pd.DataFrame(results_list)

    return descriptive_stats_df, comparison_metrics_df

# ==============================================================================
# --- EXAMPLE USAGE (This part remains the same) ---
# ==============================================================================
if __name__ == '__main__':
    # A helper function to run and display results neatly
    def run_scenario(scenario_name, s1, s2):
        print(f"--- {scenario_name} ---")
        desc_df, comps_df = compare_distributions(s1, s2, bins=40)
        
        # Check if DataFrames were returned
        if desc_df is not None:
            print("\n>>> Descriptive Statistics:")
            display(desc_df)
        
        if comps_df is not None:
            print("\n>>> Comparison Metrics:")
            display(comps_df)
        print("\n" + "="*70 + "\n")


    # --- Scenario 1: Two similar distributions ---
    np.random.seed(42)
    data1 = pd.Series(np.random.normal(loc=100, scale=10, size=1000), name="Product A Sales")
    data2_base = pd.Series(np.random.normal(loc=102, scale=12, size=1000), name="Product B Sales")
    data2_mixed = pd.concat([data2_base, pd.Series(['-','err', None, '110.5'])]).reset_index(drop=True)
    data2_mixed.name = "Product B Sales (with errors)"
    run_scenario("SCENARIO 1: COMPARING TWO SIMILAR DISTRIBUTIONS", data1, data2_mixed)


    # --- Scenario 2: Two clearly different distributions ---
    np.random.seed(0)
    data3 = pd.Series(np.random.normal(loc=80, scale=15, size=1000), name="Warehouse 1 Temp")
    data4 = pd.Series(np.random.lognormal(mean=4.5, sigma=0.5, size=1000), name="Warehouse 2 Temp")
    run_scenario("SCENARIO 2: COMPARING TWO DIFFERENT DISTRIBUTIONS", data3, data4)
