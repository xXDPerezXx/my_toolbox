"""
Enhanced Univariate Analysis (v2)

This file contains two main pieces:

1) analyze_univariate_v2(df, ...):
   - Performs a thorough per-column univariate analysis.
   - Returns a pandas DataFrame (one row per column) with:
       ['column_name', 'dtype_detected', 'distribution_count',
        'distribution_percentage', 'column_stats', 'insights',
        'insights_explanation']
   - All expensive computations are parameterized and there is a `mode` toggle
     ('fast' or 'full') to control depth.

   The docstring below is intentionally verbose: each parameter is documented,
   default is shown, and the effect of increasing/decreasing numeric defaults is explained.

2) append_analysis_to_excel(df_analysis, table_name, excel_path, verbose=True):
   - Writes/appends the analysis DataFrame to an Excel workbook.
   - Auto-versions sheet names (e.g., `table`, `table_v2`, `table_v3`) if needed.
   - Keeps IO isolated (so analysis can always be inspected in-memory).
"""

import pandas as pd
import numpy as np
import math
import re
import os
from collections import Counter
from datetime import datetime, timedelta

# -------------------------
# Formatting helpers
# -------------------------
def _fmt_int(x: int) -> str:
    if pd.isna(x):
        return "NaN"
    return f"{int(x):,}"

def _fmt_float(x: float, decimals: int = 3) -> str:
    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):
        return "NaN"
    return f"{x:,.{decimals}f}"

def _fmt_pct(x: float, decimals: int = 3) -> str:
    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):
        return "NaN"
    return f"{x*100:.{decimals}f}%"

def _safe_div(a, b):
    try:
        return a / b
    except Exception:
        return np.nan

def _calc_entropy_from_counts(counts):
    counts = np.asarray(counts, dtype=float)
    total = counts.sum()
    if total <= 0:
        return 0.0
    probs = counts / total
    probs = probs[probs > 0]
    return float(-(probs * np.log2(probs)).sum())

def _gini_coefficient_from_counts(counts):
    # Gini from frequency distribution
    arr = np.array(sorted(counts))
    n = arr.size
    if n == 0:
        return 0.0
    cum = np.cumsum(arr, dtype=float)
    sum_ = cum[-1]
    if sum_ == 0:
        return 0.0
    index = np.arange(1, n+1)
    gini = (2.0 * np.sum(index * arr) - (n+1) * sum_) / (n * sum_)
    return float(gini)

def _clean_sheet_name(name: str, max_len: int = 31) -> str:
    cleaned = re.sub(r'[:\\\/\?\*\[\]]', '_', str(name))
    if len(cleaned) > max_len:
        cleaned = cleaned[:max_len]
    return cleaned

# -------------------------
# Core analysis function
# -------------------------
def analyze_univariate_v2(
    df: pd.DataFrame,
    top_k: int = 25,
    mode: str = "full",
    detect_datetime_threshold: float = 0.80,
    detect_numeric_threshold: float = 0.90,
    zero_pad_detection_threshold: float = 0.90,
    detect_zero_padding_min_length: int = 2,
    cardinality_medium_pct: float = 0.05,
    id_like_unique_ratio: float = 0.99,
    mostly_null_threshold: float = 0.70,
    dominant_value_threshold: float = 0.90,
    uniform_entropy_ratio_threshold: float = 0.95,
    rounded_values_pct: float = 0.80,
    rounded_decimals_pct: float = 0.8,
    binary_tolerance_pct: float = 1.0,
    sparse_threshold: float = 0.80,
    many_singletons_pct: float = 0.5,
    gini_high_threshold: float = 0.6,
    pct_topk_coverage_threshold: float = 0.8,
    near_zero_variance_threshold: float = 1e-8,
    recent_days_cutoff: int = 90,
    format_numeric_decimals: int = 3,
    format_percent_decimals: int = 3,
    verbose: bool = True,
    progress_print_every: int = 50,
) -> pd.DataFrame:
    """
    analyze_univariate_v2

    Performs a detailed univariate summary for each column of `df`.

    Returns
    -------
    pd.DataFrame
        One row per input column with the columns:
          - column_name
          - dtype_detected
          - distribution_count
          - distribution_percentage
          - column_stats
          - insights
          - insights_explanation

    Parameters (with defaults and effect descriptions)
    -------------------------------------------------
    df : pandas.DataFrame
        Input dataframe to analyze. The function assumes the DataFrame fits in memory.

    top_k : int, default=25
        Number of top unique values to display in distribution columns. The rest are aggregated as 'OTHER'.
        - Increasing top_k: shows more of the distribution in-cell (longer strings); increases CPU/memory slightly.
        - Decreasing top_k: shorter outputs, faster rendering of the summary.

    mode : {'full','fast'}, default='full'
        'full' computes all metrics (entropy, tokenization, Gini, multimodality heuristics).
        'fast' skips expensive operations (deep tokenization, some text entropy, multimodality heuristics).
        - Use 'fast' for very wide or extremely large tables for a quicker scan.
        - Use 'full' for comprehensive profiling.

    detect_datetime_threshold : float in (0,1], default=0.80
        For object/string columns: if >= this fraction of non-null values parse as datetimes,
        the column will be treated as datetime.
        - Increasing threshold: requires more values to be parseable ‚Üí fewer columns coerced to datetime.
        - Decreasing threshold: more permissive datetime detection.

    detect_numeric_threshold : float in (0,1], default=0.90
        For object/string columns: if >= this fraction of non-null values coerce to numeric without NaN,
        the column will be treated as numeric.
        - Increase to be stricter (fewer coerces). Decrease to coerce more columns.

    zero_pad_detection_threshold : float in (0,1], default=0.90
        Fraction of non-null strings that must start with '0' and share a consistent length to signal zero-padding.
        - Higher value requires very consistent zero-padding to detect.

    detect_zero_padding_min_length : int, default=2
        Minimum length for zero-padded strings to consider (ignore single-character strings).

    cardinality_medium_pct : float in (0,1], default=0.05
        If n_unique / n_nonnull > cardinality_medium_pct ‚Üí label high cardinality.
        - Reduce this threshold to flag more columns as high-cardinality; increase to be more conservative.

    id_like_unique_ratio : float in (0,1], default=0.99
        If n_unique / n_nonnull >= this ratio ‚Üí tag `id_like`.
        - Increase toward 1.0 to require near-perfect uniqueness; decrease to consider more columns as ID-like.

    mostly_null_threshold : float in (0,1], default=0.70
        If n_null / total_rows >= this ‚Üí tag `mostly_null`.
        - Increase to be stricter (fewer mostly_null flags). Decrease to mark columns as mostly_null more often.

    dominant_value_threshold : float in (0,1], default=0.90
        If top1_count / n_nonnull >= this ‚Üí tag `dominant_value`.
        - Increasing makes the rule stricter (need stronger dominance).

    uniform_entropy_ratio_threshold : float in (0,1], default=0.95
        If entropy/log2(n_unique) >= this ‚Üí tag `uniform_distribution`.
        - Increase toward 1: stricter detection of near-uniform distributions.

    rounded_values_pct : float in (0,1], default=0.80
        For numeric dtype, fraction of values with zero fractional part required to tag `rounded_values`.
        - Increase to require more roundedness for detection.

    rounded_decimals_pct : float in (0,1], default=0.8
        For numeric dtype, fraction of sampled values that share the same number of decimal places (mode)
        required to tag `rounded_to_N_decimals`.

    binary_tolerance_pct : float in (0,1], default=1.0
        For `binary_flag`, required coverage of the two dominant values among non-null rows.
        - Set <1.0 to tolerate noise.

    sparse_threshold : float in (0,1], default=0.80
        If n_null / total_rows >= this or top1_pct_nonnull >= 0.9 ‚Üí tag `sparse`.

    many_singletons_pct : float in (0,1], default=0.5
        If fraction of unique values that occur exactly once >= this ‚Üí tag `many_singletons`.

    gini_high_threshold : float in (0,1], default=0.6
        Gini >= this tags `gini_high`.
        - High Gini indicates heavy concentration (few values dominate).

    pct_topk_coverage_threshold : float in (0,1], default=0.8
        If top_k values cover >= this fraction of rows ‚Üí tag `pct_topk_coverage`.

    near_zero_variance_threshold : float, default=1e-8
        Numeric variance threshold below which `near_zero_variance` is flagged.
        - Lower values make the rule stricter.

    recent_days_cutoff : int (days), default=90
        For datetime columns: fraction of timestamps within last `recent_days_cutoff` days used to flag `very_recent`.

    format_numeric_decimals : int, default=3
        Number of decimal places in numeric formatting in output strings (e.g., means, gini).

    format_percent_decimals : int, default=3
        Number of decimal places for percent formatting (e.g., 12.345%).

    verbose : bool, default=True
        Print progress logs per column if True.

    progress_print_every : int, default=50
        How often to print a progress line when verbose=True.

    Behavior notes
    --------------
    - The function returns the analysis DataFrame even if the Excel writing fails (Excel write is separate).
    - Many thresholds are configurable; each affects sensitivity (described above).
    - 'fast' mode avoids expensive text tokenization and some heavy heuristics; 'full' mode computes everything.

    Returns
    -------
    pd.DataFrame
        One-row-per-column analysis DataFrame.
    """

    if mode not in {"full", "fast"}:
        raise ValueError("mode must be 'full' or 'fast'")

    total_rows = len(df)
    results = []
    cols = list(df.columns)
    ncols = len(cols)

    for idx, col in enumerate(cols, start=1):
        if verbose and (idx == 1 or idx % progress_print_every == 0 or idx == ncols):
            print(f"[{idx}/{ncols}] Analyzing column: {col}")

        s = df[col]
        col_nonnull = s.dropna()
        n_nonnull = int(col_nonnull.shape[0])
        n_null = total_rows - n_nonnull
        pct_null = _safe_div(n_null, total_rows)

        # dtype detection (datetime first, numeric next)
        is_datetime = False; coerced_dt = None
        if s.dtype.kind in ('M',):
            is_datetime = True
            coerced_dt = pd.to_datetime(s, errors='coerce')
            dtype_detected = "datetime"
        else:
            dtype_detected = None
            if (s.dtype == object) or pd.api.types.is_string_dtype(s):
                if n_nonnull > 0:
                    parsed = pd.to_datetime(s, errors='coerce')
                    if parsed.notna().sum() / max(1, n_nonnull) >= detect_datetime_threshold:
                        is_datetime = True
                        coerced_dt = parsed
                        dtype_detected = "datetime"

        # numeric detection / coercion
        is_numeric = False; coerced_num = None
        if pd.api.types.is_numeric_dtype(s):
            is_numeric = True
            coerced_num = s.astype(float)
            dtype_detected = dtype_detected or "numeric"
        else:
            if (s.dtype == object) or pd.api.types.is_string_dtype(s):
                coerced = pd.to_numeric(s, errors='coerce')
                if coerced.notna().sum() / max(1, n_nonnull) >= detect_numeric_threshold:
                    is_numeric = True
                    coerced_num = coerced
                    dtype_detected = dtype_detected or "numeric"

        if is_datetime:
            dtype_detected = "datetime"
        elif is_numeric:
            dtype_detected = "numeric"
        else:
            if pd.api.types.is_bool_dtype(s):
                dtype_detected = "boolean"
            elif pd.api.types.is_integer_dtype(s):
                dtype_detected = dtype_detected or "integer"
            else:
                dtype_detected = dtype_detected or "text"

        # value counts and distribution
        try:
            value_counts = col_nonnull.value_counts(dropna=True)
        except Exception:
            value_counts = col_nonnull.astype(str).value_counts(dropna=True)

        n_unique = int(value_counts.shape[0])
        pct_unique = _safe_div(n_unique, max(1, n_nonnull))
        top_k_local = max(1, int(top_k))
        top_counts = value_counts.iloc[:top_k_local]
        other_count = value_counts.iloc[top_k_local:].sum() if n_unique > top_k_local else 0

        # build distribution strings
        dist_count_items = []
        dist_pct_items = []
        for val, cnt in top_counts.items():
            pct = _safe_div(cnt, max(1, total_rows))
            val_str = str(val)
            dist_count_items.append(f"{val_str} - {_fmt_int(int(cnt))}")
            dist_pct_items.append(f"{val_str} - {_fmt_pct(pct, format_percent_decimals)}")
        if other_count > 0:
            dist_count_items.append(f"OTHER - {_fmt_int(int(other_count))}")
            dist_pct_items.append(f"OTHER - {_fmt_pct(_safe_div(other_count, total_rows), format_percent_decimals)}")

        distribution_count = "; ".join(dist_count_items)
        distribution_percentage = "; ".join(dist_pct_items)

        # entropy & gini
        entropy_bits = None
        entropy_interpretation = ""
        if mode == "full" or n_unique <= 5000:
            if n_unique > 0:
                entropy_bits = _calc_entropy_from_counts(value_counts.values)
                max_entropy = math.log2(n_unique) if n_unique > 0 else 0.0
                entropy_ratio = _safe_div(entropy_bits, max_entropy) if max_entropy > 0 else 0.0
                if entropy_bits <= 1.0:
                    entropy_interpretation = " (very low diversity)"
                elif entropy_bits <= 3.0:
                    entropy_interpretation = " (low diversity)"
                elif entropy_bits <= 8.0:
                    entropy_interpretation = " (moderate diversity)"
                else:
                    entropy_interpretation = " (high diversity)"
            else:
                entropy_bits = 0.0
                entropy_interpretation = " (no diversity)"

        gini = _gini_coefficient_from_counts(value_counts.values) if n_unique > 0 else 0.0

        # singleton ratios and top-k coverage
        singleton_count = int((value_counts == 1).sum())
        singleton_ratio_unique = _safe_div(singleton_count, max(1, n_unique))
        singleton_ratio_total = _safe_div(value_counts[value_counts == 1].sum(), max(1, total_rows))
        topk_coverage = _safe_div(top_counts.sum(), max(1, total_rows))

        # memory usage
        try:
            memory_mb = s.memory_usage(deep=True) / (1024.0 ** 2)
        except Exception:
            memory_mb = np.nan

        # build column_stats (start)
        cs_parts = []
        cs_parts.append(f"total_count={_fmt_int(total_rows)}")
        cs_parts.append(f"n_nonnull={_fmt_int(n_nonnull)} ({_fmt_pct(_safe_div(n_nonnull, total_rows), format_percent_decimals)})")
        cs_parts.append(f"n_null={_fmt_int(n_null)} ({_fmt_pct(pct_null, format_percent_decimals)})")
        cs_parts.append(f"n_unique={_fmt_int(n_unique)} ({_fmt_pct(pct_unique, format_percent_decimals)})")
        if n_unique <= 50:
            cardinality_label = "low"
        else:
            cardinality_label = "high" if (pct_unique > cardinality_medium_pct) else "medium"
        cs_parts.append(f"cardinality={cardinality_label}")
        if entropy_bits is not None:
            cs_parts.append(f"entropy={_fmt_float(entropy_bits, format_numeric_decimals)} bits{entropy_interpretation}")
        cs_parts.append(f"gini={_fmt_float(gini, format_numeric_decimals)}")
        cs_parts.append(f"singleton_ratio_unique={_fmt_float(singleton_ratio_unique, format_numeric_decimals)}")
        cs_parts.append(f"top{top_k}_coverage={_fmt_pct(topk_coverage, format_percent_decimals)}")
        cs_parts.append(f"memory_mb={_fmt_float(memory_mb, format_numeric_decimals)}")

        # insights and explanations
        insights = []
        insights_explanations = []

        # empty/constant
        if n_unique == 0:
            insights.append("empty_column")
            insights_explanations.append("empty_column: No non-null values observed in this column.")
        if n_unique == 1:
            insights.append("constant_value")
            insights_explanations.append(f"constant_value: This column has a single unique value ({value_counts.index[0]}).")

        # mostly null
        if pct_null >= mostly_null_threshold:
            insights.append("mostly_null")
            insights_explanations.append(f"mostly_null: {_fmt_pct(pct_null, format_percent_decimals)} of rows are null; highly sparse.")

        # dominant value (by non-null)
        top1_count = int(value_counts.iloc[0]) if n_unique > 0 else 0
        top1_pct_nonnull = _safe_div(top1_count, max(1, n_nonnull)) if n_nonnull > 0 else 0.0
        if top1_pct_nonnull >= dominant_value_threshold:
            insights.append(f"dominant_value ({_fmt_pct(top1_pct_nonnull, format_percent_decimals)})")
            insights_explanations.append(
                f"dominant_value: The top value '{value_counts.index[0]}' covers {_fmt_pct(top1_pct_nonnull, format_percent_decimals)} of non-null rows."
            )

        # cardinality-based
        if n_unique <= 50:
            insights.append("low_cardinality")
            insights_explanations.append("low_cardinality: Column has <= 50 unique values; treat as categorical candidate.")
        if pct_unique > cardinality_medium_pct:
            insights.append("high_cardinality")
            insights_explanations.append(f"high_cardinality: Unique values represent {_fmt_pct(pct_unique, format_percent_decimals)} of non-null rows.")
        if n_nonnull > 0 and (n_unique / n_nonnull) >= id_like_unique_ratio:
            insights.append(f"id_like ({_fmt_pct(_safe_div(n_unique, n_nonnull), format_percent_decimals)})")
            insights_explanations.append(f"id_like: {_fmt_pct(_safe_div(n_unique, n_nonnull), format_percent_decimals)} unique ratio suggests identifier-like values.")

        # many_singletons
        if singleton_ratio_unique >= many_singletons_pct:
            insights.append(f"many_singletons ({_fmt_pct(singleton_ratio_unique, format_percent_decimals)})")
            insights_explanations.append(f"many_singletons: {_fmt_pct(singleton_ratio_unique, format_percent_decimals)} of unique values occur exactly once; long-tail distribution.")

        # gini_high
        if gini >= gini_high_threshold:
            insights.append(f"gini_high ({_fmt_float(gini, format_numeric_decimals)})")
            insights_explanations.append(f"gini_high: Gini coefficient {_fmt_float(gini, format_numeric_decimals)} indicates heavy concentration of counts in a few values.")

        # top-k coverage
        if topk_coverage >= pct_topk_coverage_threshold:
            insights.append(f"pct_top{top_k}_coverage ({_fmt_pct(topk_coverage, format_percent_decimals)})")
            insights_explanations.append(f"pct_top{top_k}_coverage: Top {top_k} values cover {_fmt_pct(topk_coverage, format_percent_decimals)} of rows.")

        # monotonicity checks for numeric/datetime
        if is_numeric or is_datetime:
            arr = None
            if is_numeric and coerced_num is not None:
                arr = coerced_num.dropna().astype(float).values
            if is_datetime and coerced_dt is not None:
                # convert to integer epoch (ns) to test ordering
                arr = coerced_dt.dropna().astype('int64').values
            if arr is not None and arr.size > 1:
                non_decreasing = np.all(np.diff(arr) >= 0)
                non_increasing = np.all(np.diff(arr) <= 0)
                if non_decreasing:
                    insights.append("monotonic_increasing")
                    insights_explanations.append("monotonic_increasing: Values are non-decreasing across the column (may indicate ordered or incremental values).")
                elif non_increasing:
                    insights.append("monotonic_decreasing")
                    insights_explanations.append("monotonic_decreasing: Values are non-increasing across the column.")

        # numeric-specific metrics & tags
        if is_numeric and coerced_num is not None:
            arrs = coerced_num.dropna().astype(float)
            if arrs.size > 0:
                mean = float(arrs.mean()); median = float(arrs.median()); std = float(arrs.std(ddof=0))
                q1 = float(arrs.quantile(0.25)); q3 = float(arrs.quantile(0.75)); iqr = q3 - q1
                mn = float(arrs.min()); mx = float(arrs.max())
                p1 = float(arrs.quantile(0.01)); p99 = float(arrs.quantile(0.99))
                p99_p1_ratio = _safe_div(p99, p1) if p1 != 0 else np.nan
                skewness = float(arrs.skew()); kurt = float(arrs.kurt()); mad = float((arrs - arrs.median()).abs().median())
                variance = float(arrs.var())
                outlier_mask = (arrs < (q1 - 1.5*iqr)) | (arrs > (q3 + 1.5*iqr))
                outlier_count = int(outlier_mask.sum()); outlier_pct = _safe_div(outlier_count, arrs.size)

                cs_parts.extend([
                    f"mean={_fmt_float(mean, format_numeric_decimals)}",
                    f"median={_fmt_float(median, format_numeric_decimals)}",
                    f"std={_fmt_float(std, format_numeric_decimals)}",
                    f"min={_fmt_float(mn, format_numeric_decimals)}",
                    f"q1={_fmt_float(q1, format_numeric_decimals)}",
                    f"q3={_fmt_float(q3, format_numeric_decimals)}",
                    f"iqr={_fmt_float(iqr, format_numeric_decimals)}",
                    f"max={_fmt_float(mx, format_numeric_decimals)}",
                    f"p1={_fmt_float(p1, format_numeric_decimals)}",
                    f"p99={_fmt_float(p99, format_numeric_decimals)}",
                    f"p99_p1_ratio={_fmt_float(p99_p1_ratio, format_numeric_decimals)}",
                    f"skewness={_fmt_float(skewness, format_numeric_decimals)}",
                    f"kurtosis={_fmt_float(kurt, format_numeric_decimals)}",
                    f"mad={_fmt_float(mad, format_numeric_decimals)}",
                    f"variance={_fmt_float(variance, format_numeric_decimals)}",
                ])

                # near_zero_variance
                if variance <= near_zero_variance_threshold:
                    insights.append(f"near_zero_variance ({_fmt_float(variance, format_numeric_decimals)})")
                    insights_explanations.append(f"near_zero_variance: Variance {_fmt_float(variance, format_numeric_decimals)} is very small; effectively constant.")

                # small_range
                if (mx - mn) < 1e-6:
                    insights.append("small_range")
                    insights_explanations.append("small_range: Numeric range is extremely small.")

                # many_outliers
                if outlier_count > 0:
                    insights.append(f"many_outliers ({_fmt_pct(outlier_pct, format_percent_decimals)})")
                    insights_explanations.append(f"many_outliers: {_fmt_int(outlier_count)} outliers detected ({_fmt_pct(outlier_pct, format_percent_decimals)} of non-null).")

                # rounded decimals detection (sample for performance)
                decimal_places = []
                vals = arrs.values
                sample = vals if vals.size <= 10000 else vals[:10000]
                for v in sample:
                    s_val = f"{v:.6f}"
                    if '.' in s_val:
                        frac = s_val.split('.')[1].rstrip('0')
                        decimal_places.append(len(frac))
                    else:
                        decimal_places.append(0)
                if len(decimal_places) > 0:
                    most_common_dec = Counter(decimal_places).most_common(1)[0]
                    dec_place_mode, dec_count = most_common_dec
                    if dec_count / max(1, len(decimal_places)) >= rounded_decimals_pct:
                        insights.append(f"rounded_to_{dec_place_mode}_decimals ({_fmt_pct(dec_count/len(decimal_places), format_percent_decimals)})")
                        insights_explanations.append(f"rounded_to_{dec_place_mode}_decimals: {_fmt_pct(dec_count/len(decimal_places), format_percent_decimals)} of sampled values show {dec_place_mode} decimal places.")

                # skewness
                if abs(skewness) > 1:
                    insights.append("highly_skewed")
                    insights_explanations.append(f"highly_skewed: Skewness = {_fmt_float(skewness, format_numeric_decimals)} indicates skewed distribution.")

        # datetime-specific checks
        if is_datetime and coerced_dt is not None:
            dt = coerced_dt.dropna()
            if dt.shape[0] > 0:
                dt_min = dt.min(); dt_max = dt.max()
                range_days = (dt_max - dt_min).days
                cs_parts.extend([
                    f"min={dt_min.isoformat(sep=' ')}",
                    f"max={dt_max.isoformat(sep=' ')}",
                    f"range_days={_fmt_int(range_days)}"
                ])
                now = pd.Timestamp.now()
                recent_pct = _safe_div((dt >= (now - pd.Timedelta(days=recent_days_cutoff))).sum(), dt.shape[0])
                if recent_pct >= 0.5:
                    insights.append(f"very_recent ({_fmt_pct(recent_pct, format_percent_decimals)})")
                    insights_explanations.append(f"very_recent: {_fmt_pct(recent_pct, format_percent_decimals)} of timestamps are within the last {recent_days_cutoff} days.")
                if range_days < 7:
                    insights.append("short_time_span")
                    insights_explanations.append(f"short_time_span: Date range is only {range_days} days; may be a subset or recent sample.")
                top_hour = dt.dt.hour.value_counts().max() / dt.shape[0]
                top_dow = dt.dt.day_name().value_counts().max() / dt.shape[0]
                if top_hour >= 0.2:
                    insights.append("seasonal_strength")
                    insights_explanations.append("seasonal_strength: Strong hourly pattern detected.")
                elif top_dow >= 0.2:
                    insights.append("seasonal_strength")
                    insights_explanations.append("seasonal_strength: Strong day-of-week pattern detected.")

        # text-specific checks
        if not is_numeric and not is_datetime:
            col_text = col_nonnull.astype(str)
            if col_text.shape[0] > 0:
                lengths = col_text.str.len()
                avg_len = int(round(float(lengths.mean())))
                median_len = int(round(float(lengths.median())))
                cs_parts.append(f"avg_length={avg_len}")
                cs_parts.append(f"median_length={median_len}")

                # non-ascii percent
                non_ascii_pct = _safe_div((col_text.str.encode('utf-8').apply(lambda x: any(b > 127 for b in x))).sum(), col_text.shape[0])
                cs_parts.append(f"non_ascii_pct={_fmt_pct(non_ascii_pct, format_percent_decimals)}")

                # mostly numeric strings
                numeric_like = col_text.str.match(r"^\d+$")
                numeric_like_pct = _safe_div(numeric_like.sum(), col_text.shape[0])
                if numeric_like_pct >= 0.5:
                    insights.append(f"mostly_numeric_strings ({_fmt_pct(numeric_like_pct, format_percent_decimals)})")
                    insights_explanations.append(f"mostly_numeric_strings: {_fmt_pct(numeric_like_pct, format_percent_decimals)} of non-null values are numeric-only strings.")

                # uuid-like pattern
                uuid_mask = col_text.str.match(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$", case=False)
                uuid_pct = _safe_div(uuid_mask.sum(), col_text.shape[0])
                if uuid_pct >= 0.01:
                    insights.append(f"pattern_uuid ({_fmt_pct(uuid_pct, format_percent_decimals)})")
                    insights_explanations.append(f"pattern_uuid: {_fmt_pct(uuid_pct, format_percent_decimals)} of values look like UUIDs.")

                # zero-padding detection
                starts_with_zero = col_text.str.match(r"^0\d+")
                if starts_with_zero.sum() / max(1, col_text.shape[0]) >= zero_pad_detection_threshold:
                    lengths_with_zero = col_text[starts_with_zero].str.len()
                    if lengths_with_zero.nunique() == 1 and lengths_with_zero.iloc[0] >= detect_zero_padding_min_length:
                        cs_parts.append("zero_padding_detected=True")
                        insights.append("zero_padding")
                        insights_explanations.append("zero_padding: Leading zeros with consistent length detected; preserve formatting if this is an ID/code.")

                # deep text metrics (only in full mode)
                if mode == "full":
                    tokens = col_text.str.split().explode()
                    token_counts = tokens.value_counts()
                    n_unique_tokens = int(token_counts.shape[0]) if token_counts.shape[0] > 0 else 0
                    pct_unique_tokens = _safe_div(n_unique_tokens, max(1, tokens.shape[0])) if token_counts.shape[0] > 0 else 0.0
                    cs_parts.append(f"n_unique_tokens={_fmt_int(n_unique_tokens)} ({_fmt_pct(pct_unique_tokens, format_percent_decimals)})")
                    token_entropy = _calc_entropy_from_counts(token_counts.values) if token_counts.shape[0] > 0 else 0.0
                    cs_parts.append(f"token_entropy={_fmt_float(token_entropy, format_numeric_decimals)}")
                    if token_entropy > 4.0:
                        insights.append("high_token_entropy")
                        insights_explanations.append("high_token_entropy: Token-level entropy is high, suggesting freeform text.")

                # long tail text heuristic
                if n_unique > 1000 and topk_coverage < 0.2:
                    insights.append("long_tail_text")
                    insights_explanations.append("long_tail_text: Many rare values with low top-k coverage; likely user-generated content.")

        # sparse detection
        if pct_null >= sparse_threshold or (n_unique > 0 and top1_pct_nonnull >= 0.9):
            insights.append("sparse")
            insights_explanations.append("sparse: Column is sparse or dominated by a single value.")

        # low entropy but many values
        if entropy_bits is not None and n_unique > 50:
            max_entropy = math.log2(n_unique)
            if max_entropy > 0:
                entropy_ratio = _safe_div(entropy_bits, max_entropy)
                if entropy_ratio < 0.3 and n_unique > 100:
                    insights.append("low_entropy_but_many_values")
                    insights_explanations.append("low_entropy_but_many_values: Many distinct values exist but a few dominate (power-law distribution).")

        # binary detection (with tolerance)
        if n_nonnull > 0:
            if n_unique == 2:
                two_cover = _safe_div(value_counts.iloc[:2].sum(), n_nonnull)
                if two_cover >= binary_tolerance_pct:
                    insights.append("binary_flag")
                    insights_explanations.append("binary_flag: Column has two dominant values and behaves like a binary indicator.")
            else:
                if n_unique > 2 and _safe_div(value_counts.iloc[:2].sum(), n_nonnull) >= 0.98:
                    insights.append("binary_like")
                    insights_explanations.append("binary_like: Top two values cover >=98% of non-null rows; near-binary with minor noise.")

        # finalize ordering of insights by crude strength score
        strength_scores = {}
        for tag in insights:
            score = 0.0
            if "id_like" in tag: score += 10.0
            if "mostly_null" in tag: score += 8.0
            if "dominant_value" in tag: score += 7.0
            if "gini_high" in tag:
                score += gini * 10.0
            if "many_singletons" in tag: score += 6.0
            if "sparse" in tag: score += 5.0
            if "high_token_entropy" in tag: score += 4.0
            if "highly_skewed" in tag: score += 3.0
            if "many_outliers" in tag: score += 3.0
            if "pct_top" in tag: score += 4.0
            strength_scores[tag] = score

        insights_sorted = sorted(insights, key=lambda t: strength_scores.get(t, 0.0), reverse=True)
        # insights column show tag plus compact reason when available (we built them with values)
        insights_str = " | ".join(insights_sorted)

        # build explanations ordered the same as tags
        # we map by prefix token to explanation (best-effort mapping)
        expl_map = {}
        for tag, expl in zip(insights, insights_explanations):
            key = tag.split(' ')[0].split('(')[0]
            expl_map[key] = expl
        expls_ordered = []
        for tag in insights_sorted:
            key = tag.split(' ')[0].split('(')[0]
            if key in expl_map:
                expls_ordered.append(expl_map[key])
        insights_expl_str = " | ".join(expls_ordered)

        column_stats = "; ".join(cs_parts)

        results.append({
            "column_name": col,
            "dtype_detected": dtype_detected,
            "distribution_count": distribution_count,
            "distribution_percentage": distribution_percentage,
            "column_stats": column_stats,
            "insights": insights_str,
            "insights_explanation": insights_expl_str
        })

    summary_df = pd.DataFrame(results, columns=[
        "column_name",
        "dtype_detected",
        "distribution_count",
        "distribution_percentage",
        "column_stats",
        "insights",
        "insights_explanation"
    ])
    return summary_df

# -------------------------
# Excel append helper (separate I/O)
# -------------------------
def append_analysis_to_excel(
    df_analysis: pd.DataFrame,
    table_name: str,
    excel_path: str = "univariate_analysis.xlsx",
    verbose: bool = True
) -> str:
    """
    Append df_analysis to an Excel workbook under a sheet named after `table_name`.
    If the sheet already exists, create versioned names: table_name, table_name_v2, table_name_v3, ...
    Sanitizes sheet names and enforces Excel's 31-character sheet name limit.

    Returns:
        final_sheet_name : str
    """
    base_name = _clean_sheet_name(table_name, max_len=31)
    if os.path.exists(excel_path):
        try:
            import openpyxl
            wb = openpyxl.load_workbook(excel_path)
            existing_sheets = wb.sheetnames
        except Exception as e:
            raise RuntimeError(f"Unable to read existing Excel workbook '{excel_path}': {e}")
    else:
        wb = None
        existing_sheets = []

    candidate = base_name
    if candidate in existing_sheets:
        # find max existing _vN suffix and increment
        pattern = re.compile(re.escape(base_name) + r"(?:_v(\d+))?$")
        max_v = 1
        for s in existing_sheets:
            m = pattern.match(s)
            if m:
                if m.group(1):
                    try:
                        max_v = max(max_v, int(m.group(1)))
                    except Exception:
                        pass
                else:
                    max_v = max(max_v, 1)
        next_v = max_v + 1
        suffix = f"_v{next_v}"
        if len(base_name) + len(suffix) > 31:
            trim_len = 31 - len(suffix)
            base_trimmed = base_name[:trim_len]
            candidate = base_trimmed + suffix
        else:
            candidate = base_name + suffix

        # ensure non-collision
        v_try = next_v
        while candidate in existing_sheets:
            v_try += 1
            suffix = f"_v{v_try}"
            if len(base_name) + len(suffix) > 31:
                trim_len = 31 - len(suffix)
                base_trimmed = base_name[:trim_len]
                candidate = base_trimmed + suffix
            else:
                candidate = base_name + suffix

    # write using pandas ExcelWriter
    try:
        if wb is None:
            with pd.ExcelWriter(excel_path, engine="openpyxl", mode="w") as writer:
                df_analysis.to_excel(writer, sheet_name=candidate, index=False)
        else:
            # mode 'a' append; using if_sheet_exists='replace' for safety but candidate is unique
            with pd.ExcelWriter(excel_path, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                df_analysis.to_excel(writer, sheet_name=candidate, index=False)
    except TypeError:
        # older pandas fallback
        from openpyxl import Workbook
        if wb is None:
            wb = Workbook()
            wb.remove(wb.active)
        from openpyxl.utils.dataframe import dataframe_to_rows
        ws = wb.create_sheet(title=candidate)
        for r in dataframe_to_rows(df_analysis, index=False, header=True):
            ws.append(r)
        wb.save(excel_path)
    except Exception as e:
        raise RuntimeError(f"Failed to write analysis to Excel: {e}")

    if verbose:
        print(f"Wrote analysis to '{excel_path}' sheet '{candidate}'")
    return candidate

# -------------------------
# Example usage (small toy example)
# -------------------------
if __name__ == "__main__":
    # Create a tiny toy DataFrame to demo usage
    N = 2000
    np.random.seed(0)
    df_demo = pd.DataFrame({
        "id": [f"{i:08d}" for i in range(N)],  # zero-padded
        "age": np.random.choice([20,25,30,35,40,np.nan], size=N, p=[0.17,0.17,0.17,0.17,0.22,0.1]),
        "signup_date": pd.date_range(start="2024-01-01", periods=N, freq="H"),
        "status": np.random.choice(["active","inactive","pending","active"], size=N, p=[0.7,0.15,0.1,0.05]),
        "notes": np.random.choice(["ok","", "needs review", "urgent", "follow-up", "üëç"], size=N, p=[0.55,0.25,0.08,0.04,0.04,0.04]),
        "score": np.concatenate([np.random.normal(loc=50, scale=10, size=N-10), np.array([999]*10)])
    })

    # Run analysis (full mode)
    summary = analyze_univariate_v2(df_demo, top_k=5, mode="full", verbose=True)

    # Show top rows in console
    pd.set_option('display.max_colwidth', 200)
    print(summary.head(10).to_string(index=False))

    # Append to Excel workbook (creates or appends)
    sheet = append_analysis_to_excel(summary, table_name="demo_customer_data", excel_path="univariate_analysis.xlsx", verbose=True)
    print("Saved to sheet:", sheet)
