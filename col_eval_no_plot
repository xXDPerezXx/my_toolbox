# Evaluating one column, no plotting, just insights
# Enhanced version of evaluate_column with better readability and insight
import pandas as pd
import numpy as np
import re
from typing import List, Dict, Any
from scipy.stats import skew, kurtosis, entropy as scipy_entropy
from collections import Counter
import math


def evaluate_column_readable(df: pd.DataFrame, column_name: str, top_n: int = 5) -> pd.DataFrame:
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' not found in DataFrame.")

    col = df[column_name]
    total = len(df)
    non_null = col.dropna()
    missing_pct = (total - non_null.count()) / total * 100 if total > 0 else 0
    unique = non_null.nunique()
    unique_pct = unique / non_null.count() * 100 if non_null.count() > 0 else 0
    memory_usage = col.memory_usage(deep=True) / (1024 ** 2)

    # Inferred type
    inferred_type, confidence = _infer_type(col)

    # Quality assessment
    quality_score, quality_issues = _quality_issues(col)

    # Pattern detection (always populate)
    patterns = _detect_patterns_readable(col, inferred_type)

    # Distribution summary
    distribution = _distribution_summary(col, total, top_n)

    # Summary stats
    stats_dict, stats_str = _summary_stats(col, inferred_type)

    # Insights
    insights = _generate_readable_insights(col, inferred_type, quality_score, quality_issues, patterns, stats_dict)

    # Format final output for readability
    return pd.DataFrame([{
        "Column_Name": column_name,
        "Data_Quality_Score": f"{quality_score}/100",
        "Quality_Issues": "; ".join(quality_issues) if quality_issues else "None detected.",
        "Patterns_Detected": "; ".join(patterns) if patterns else "No notable patterns detected.",
        "Distribution_Analysis": distribution,
        "Summary_Statistics": stats_str,
        "Memory_Usage_MB": round(memory_usage, 4),
        "Insights_And_Recommendations": "; ".join(insights) if insights else "No specific recommendations."
    }])


def _summary_stats(col: pd.Series, inferred_type: str) -> tuple[dict, str]:
    non_null = col.dropna()
    stats = {
        "total_count": len(col),
        "non_null_count": non_null.count(),
        "null_count": col.isna().sum(),
        "unique_count": non_null.nunique(),
        "unique_pct": (non_null.nunique() / non_null.count() * 100) if non_null.count() > 0 else 0.0,
        "negative_count": (non_null < 0).sum() if pd.api.types.is_numeric_dtype(non_null) else 'N/A'
    }

    summary = [
        f"Total: {stats['total_count']}",
        f"Non-null: {stats['non_null_count']}",
        f"Null: {stats['null_count']}",
        f"Unique: {stats['unique_count']} ({stats['unique_pct']:.1f}%)",
        f"Negative Values: {stats['negative_count']}"
    ]

    if inferred_type in ["float", "integer"] and len(non_null) > 1:
        desc = non_null.describe()
        stats.update({
            "min": desc["min"],
            "max": desc["max"],
            "mean": desc["mean"],
            "std": desc["std"],
            "skewness": skew(non_null),
            "kurtosis": kurtosis(non_null)
        })

        skew_val = stats['skewness']
        kurt_val = stats['kurtosis']
        if abs(skew_val) < 0.5:
            skew_note = "approximately symmetric"
        elif skew_val > 0:
            skew_note = "right-skewed"
        else:
            skew_note = "left-skewed"

        if kurt_val > 3:
            kurt_note = "leptokurtic (heavy tails)"
        elif kurt_val < 3:
            kurt_note = "platykurtic (light tails)"
        else:
            kurt_note = "mesokurtic (normal-like)"

        summary.extend([
            f"Min: {desc['min']:.2f}",
            f"Max: {desc['max']:.2f}",
            f"Mean: {desc['mean']:.2f}",
            f"Std: {desc['std']:.2f}",
            f"Skewness: {skew_val:.2f} ({skew_note})",
            f"Kurtosis: {kurt_val:.2f} ({kurt_note})"
        ])
        stats['skew_note'] = skew_note
        stats['kurt_note'] = kurt_note

    return stats, "; ".join(summary)

def _infer_type(col: pd.Series) -> tuple[str, float]:
    non_null = col.dropna()
    if len(non_null) == 0:
        return "empty", 1.0
    if pd.api.types.is_numeric_dtype(non_null):
        return ("float" if pd.api.types.is_float_dtype(non_null) else "integer"), 0.95
    if pd.api.types.is_datetime64_any_dtype(non_null):
        return "datetime", 0.95
    if pd.api.types.is_bool_dtype(non_null):
        return "boolean", 1.0

    sample = non_null.astype(str).sample(min(100, len(non_null)), random_state=1)
    date_regexes = [r"\d{4}-\d{2}-\d{2}", r"\d{2}/\d{2}/\d{4}"]
    if sample.str.contains('|'.join(date_regexes)).mean() > 0.6:
        return "date_string", 0.85

    avg_len = sample.str.len().mean()
    if avg_len > 15:
        return "text", 0.8
    else:
        return "categorical", 0.8


def _generate_readable_insights(col: pd.Series, inferred_type: str, score: int, issues: List[str], patterns: List[str], stats: dict) -> List[str]:
    insights = []
    non_null = col.dropna()

    entropy_val = _calculate_entropy(col)
    unique_pct = stats.get("unique_pct", 0)

    if score < 70:
        insights.append("Low data quality. Significant cleaning recommended.")
    elif score < 90:
        insights.append("Moderate data quality. Some improvements may help.")
    else:
        insights.append("High data quality. Minimal issues detected.")

    if inferred_type == "date_string":
        insights.append("Column may benefit from conversion to datetime format.")

    if inferred_type == "text" and col.nunique() > 100:
        insights.append("Text column has high cardinality. Consider summarization or vectorization.")

    if inferred_type == "categorical" and col.nunique() > 50:
        insights.append("High number of categories. Consider combining infrequent ones.")

    # Use datatype-aware uniqueness evaluation
    if inferred_type in ["integer", "float"]:
        if col.nunique() > 100:
            insights.append("High uniqueness — likely a continuous numeric field.")
    else:
        if unique_pct > 95:
            insights.append("Over 95% of values are unique — likely an identifier or token.")
        elif unique_pct < 5:
            insights.append("Low uniqueness — likely a label, flag, or categorical variable.")

    if inferred_type == "float":
        min_val, max_val = non_null.min(), non_null.max()
        if min_val >= 0 and max_val <= 1:
            insights.append("Numeric values bounded between 0 and 1 — may represent proportions or probabilities.")

    if "email" in "".join(patterns).lower():
        insights.append("Field may contain email addresses. Review for potential PII exposure.")

    if entropy_val < 1:
        insights.append("Low entropy — values are highly repetitive.")
    elif entropy_val > 4:
        insights.append("High entropy — likely a hashed field, UUID, or random token.")
    else:
        insights.append(f"Entropy level: {entropy_val} bits — moderate variability.")

    if 'skew_note' in stats:
        insights.append(f"Distribution is {stats['skew_note']}.")
    if 'kurt_note' in stats:
        insights.append(f"Tail shape is {stats['kurt_note']}.")

    return insights

def _quality_issues(col: pd.Series) -> tuple[int, List[str]]:
    issues = []
    total = len(col)
    non_null = col.dropna()
    score = 100

    missing_pct = (total - non_null.count()) / total * 100 if total > 0 else 0
    if missing_pct > 50:
        issues.append(f"Over 50% missing values ({missing_pct:.1f}%)")
        score -= 30
    elif missing_pct > 20:
        issues.append(f"Moderate missing values ({missing_pct:.1f}%)")
        score -= 15
    elif missing_pct > 5:
        issues.append(f"Some missing values ({missing_pct:.1f}%)")
        score -= 5

    if non_null.count() > 0:
        dup_pct = (non_null.count() - non_null.nunique()) / non_null.count() * 100
        if dup_pct > 90:
            issues.append(f"Very high duplication ({dup_pct:.1f}%)")
            score -= 20
        elif dup_pct > 50:
            issues.append(f"High duplication ({dup_pct:.1f}%)")
            score -= 10

    if pd.api.types.is_numeric_dtype(col):
        q1 = non_null.quantile(0.25)
        q3 = non_null.quantile(0.75)
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        outliers = non_null[(non_null < lower) | (non_null > upper)]
        if len(non_null) > 0:
            out_pct = len(outliers) / len(non_null) * 100
            if out_pct > 10:
                issues.append(f"Outliers present in {out_pct:.1f}% of values")
                score -= 10

    return max(score, 0), issues

def _detect_patterns_readable(col: pd.Series, inferred_type: str) -> List[str]:
    patterns = []
    non_null = col.dropna().astype(str)
    sample = non_null.sample(min(100, len(non_null)), random_state=1)

    if inferred_type in ["text", "categorical", "date_string"]:
        email_re = r"[\w.-]+@[\w.-]+\.\w+"
        phone_re = r"\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}"
        url_re = r"https?://[\w./-]+"

        if sample.str.contains(email_re).mean() > 0.3:
            patterns.append("Contains email-like strings")
        if sample.str.contains(phone_re).mean() > 0.3:
            patterns.append("Phone number format observed")
        if sample.str.contains(url_re).mean() > 0.2:
            patterns.append("Likely contains URLs")

        len_std = sample.str.len().std()
        if len_std < 2:
            patterns.append("Mostly fixed-length strings")
        elif len_std < 5:
            patterns.append("Slight variation in string length")
        else:
            patterns.append("High variation in string length")

        pattern_clusters = sample.apply(lambda x: re.sub(r"[A-Z]+", "A", re.sub(r"[a-z]+", "a", re.sub(r"\d", "9", x))))
        top_patterns = pattern_clusters.value_counts().head(3)
        for pat in top_patterns.index:
            patterns.append(f"Frequent pattern: {pat} ({top_patterns[pat]} samples)")

    return patterns

def _distribution_summary(col: pd.Series, total: int, top_n: int) -> str:
    counts = col.value_counts(dropna=True).head(top_n)
    summary = []
    for val, count in counts.items():
        pct = count / total * 100
        summary.append(f"'{val}' - {count} ({pct:.1f}%)")
    return "; ".join(summary) if summary else "No values to summarize."

def _calculate_entropy(col: pd.Series) -> float:
    non_null = col.dropna().astype(str)
    freqs = non_null.value_counts(normalize=True)
    return round(scipy_entropy(freqs, base=2), 2) if not freqs.empty else 0.0
